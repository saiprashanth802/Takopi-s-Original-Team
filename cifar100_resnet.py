# -*- coding: utf-8 -*-
"""cifar100_resnet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nd2n3rsIM9SqqriyPhudyaIZx712jvFR
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import GlobalAveragePooling2D

train=True
num_classes = 41
weight_decay = 0.0005
x_shape = [224,224,3]

def normalize(X_train,X_test):
    #this function normalize inputs for zero mean and unit variance
    # it is used when training a model.
    # Input: training set and test set
    # Output: normalized training set and test set according to the trianing set statistics.
    mean = np.mean(X_train,axis=(0,1,2,3))
    std = np.std(X_train, axis=(0, 1, 2, 3))
    print(mean)
    print(std)
    X_train = (X_train-mean)/(std+1e-7)
    X_test = (X_test-mean)/(std+1e-7)
    return X_train, X_test

def normalize_production(x):
    #this function is used to normalize instances in production according to saved training set statistics
    # Input: X - a training set
    # Output X - a normalized training set according to normalization constants.

    #these values produced during first training and are general for the standard cifar10 training set normalization
    mean = 121.936
    std = 68.389
    return (x-mean)/(std+1e-7)

def predict(x,normalize=True,batch_size=32):
    if normalize:
        x = normalize_production(x)
    return model.predict(x,batch_size)

from google.colab import drive
drive.mount('/content/drive')

def load_and_split_data(data_dir, target_size=(64,64), test_split=0.2, batch_size=32):
    """
    Load and split image dataset from Kaggle into x_train, x_test, y_train, y_test.

    Parameters:
    - data_dir: Path to the dataset directory with subfolders for each breed.
    - target_size: Tuple for image resizing (height, width).
    - test_split: Fraction of data for testing (default: 0.2).
    - batch_size: Batch size for data loading (default: 32).

    Returns:
    - x_train, x_test: NumPy arrays of training and testing images.
    - y_train, y_test: NumPy arrays of training and testing labels (one-hot encoded).
    """
    # Initialize ImageDataGenerator for preprocessing
    datagen = ImageDataGenerator(
        rescale=1./255,  # Normalize pixel values to [0,1]
        validation_split=test_split  # Split for testing
    )

    # Load training data
    train_generator = datagen.flow_from_directory(
        data_dir,
        target_size=target_size,
        batch_size=batch_size,
        class_mode='categorical',  # One-hot encoded labels for multi-class
        subset='training',
        shuffle=True,
        seed=42
    )




    # Initialize lists to store images and labels
    x_train, y_train = [], []
    x_test, y_test = [], []

    # Collect training data
    for i in range(len(train_generator)):
        images, labels = train_generator[i]
        x_train.append(images)
        y_train.append(labels)

    # Collect test

    # Concatenate batches into NumPy arrays
    x_train = np.concatenate(x_train, axis=0)
    y_train = np.concatenate(y_train, axis=0)
    x_test = np.concatenate(x_test, axis=0)
    y_test = np.concatenate(y_test, axis=0)

    return x_train, x_test, y_train, y_test

x_train, x_test, y_train, y_test = load_and_split_data(data_dir='/content/drive/MyDrive/Indian_bovine_breeds')

for i in range 41:
print(labels[i])

def identity_block(input_tensor, filters, kernel_size=3):
    filters1, filters2, filters3 = filters

    x = layers.Conv2D(filters1, 1, padding='valid')(input_tensor)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    x = layers.Conv2D(filters2, kernel_size, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    x = layers.Conv2D(filters3, 1, padding='valid')(x)
    x = layers.BatchNormalization()(x)

    x = layers.Add()([x, input_tensor])
    x = layers.Activation('relu')(x)
    return x

def conv_block(input_tensor, filters, kernel_size=3, strides=2):
    filters1, filters2, filters3 = filters

    x = layers.Conv2D(filters1, 1, strides=strides, padding='valid')(input_tensor)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    x = layers.Conv2D(filters2, kernel_size, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    x = layers.Conv2D(filters3, 1, padding='valid')(x)
    x = layers.BatchNormalization()(x)

    shortcut = layers.Conv2D(filters3, 1, strides=strides, padding='valid')(input_tensor)
    shortcut = layers.BatchNormalization()(shortcut)

    x = layers.Add()([x, shortcut])
    x = layers.Activation('relu')(x)
    return x

def ResNet50(input_shape=(64, 64, 3), classes=41):
    img_input = layers.Input(shape=input_shape)

    x = layers.Conv2D(64, 7, strides=2, padding='same')(img_input)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)

    # Stage 1
    x = conv_block(x, [64, 64, 256], strides=1)
    x = identity_block(x, [64, 64, 256])
    x = identity_block(x, [64, 64, 256])

    # Stage 2
    x = conv_block(x, [128, 128, 512])
    x = identity_block(x, [128, 128, 512])
    x = identity_block(x, [128, 128, 512])
    x = identity_block(x, [128, 128, 512])

    # Stage 3
    x = conv_block(x, [256, 256, 1024])
    x = identity_block(x, [256, 256, 1024])
    x = identity_block(x, [256, 256, 1024])
    x = identity_block(x, [256, 256, 1024])
    x = identity_block(x, [256, 256, 1024])
    x = identity_block(x, [256, 256, 1024])

    # Stage 4
    x = conv_block(x, [512, 512, 2048])
    x = identity_block(x, [512, 512, 2048])
    x = identity_block(x, [512, 512, 2048])

    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(classes, activation='softmax')(x)

    model = Model(img_input, x, name='resnet50')



    return model

model=ResNet50()

"""from google.colab import drive
drive.mount('/content/drive')
"""

from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(
    monitor='val_accuracy',
    patience=5,
    restore_best_weights=True
)
from tensorflow.keras.callbacks import ModelCheckpoint
checkpoint = ModelCheckpoint(
    'best_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    mode='max'
)
from tensorflow.keras.callbacks import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.2,
    patience=3,
    min_lr=1e-6
)
from tensorflow.keras.callbacks import TensorBoard
tensorboard = TensorBoard(log_dir='./logs', histogram_freq=1)

callbacks = [
    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),
    ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True),
    ReduceLROnPlateau(monitor='val_accuracy', factor=0.15, patience=3, min_lr=1e-6),
    TensorBoard(log_dir='./logs')
]

datagen = ImageDataGenerator(
    featurewise_center=False,  # set input mean to 0 over the dataset
    samplewise_center=False,  # set each sample mean to 0
    featurewise_std_normalization=False,  # divide inputs by std of the dataset
    samplewise_std_normalization=False,  # divide each input by its std
    zca_whitening=False,  # apply ZCA whitening
    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)
    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=True,  # randomly flip images
    vertical_flip=False)  # randomly flip images

# (std, mean, and principal components if ZCA whitening is applied).
datagen.fit(x_train)

#optimization details
adam = keras.optimizers.SGD(learning_rate=0.01,momentum=0.8)

# Compile for a single-output model
model.compile(loss='categorical_crossentropy', optimizer=adam,metrics=['accuracy'])

batch_size = 32
epochs = 200

# Fit the model using ImageDataGenerator and validation data
history = model.fit(
    datagen.flow(x_train, y_train, batch_size=batch_size),
    steps_per_epoch=x_train.shape[0] // batch_size,
    epochs=epochs,
    validation_data=(x_test, y_test), # x_test and y_test have the correct shapes now
    verbose=2,
    callbacks=callbacks
)

model.save('resnet.h5')

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

# Initialize lists to store results
results = []

# Load the best model (assuming it's saved with all layers, including custom objects if any)
#model = tf.keras.models.load_model('/content/googlenet.h5')


for i in range(100):
    # Get the test image and its true label
    image = x_test[i * 100]  # Shape: (224, 224, 3) after resizing
    true_class_one_hot = y_test[i * 100]  # Shape: (100,) - one-hot encoded vector
    true_class = np.argmax(true_class_one_hot)  # Get the index of the true class

    # Add batch dimension to the image
    image = np.expand_dims(image, axis=0)  # Shape: (1, 224, 224, 3)

    # Make prediction (model returns three outputs)
    predictions = model.predict(image)  # Returns a list: [main_output, aux_output_1, aux_output_2]
    predicted_class = np.argmax(predictions, axis=1)[0]  # Use main_output for final prediction
    confidence = np.max(predictions)  # Confidence from main_output

    # Store results in dictionary
    results.append({
        'Image_Index': i * 100,
        'Predicted_Class': cifar100_labels[predicted_class],
        'True_Class': cifar100_labels[true_class],
        'Confidence': f"{confidence:.4f}",
        'Correct': predicted_class == true_class
    })

    # Visualize the image
    plt.imshow(x_test[i * 100])  # Show original 32x32 image for visualization
    plt.title(f"Predicted: {cifar100_labels[predicted_class]}, True: {cifar100_labels[true_class]}")
    plt.show()

    # Print results
    print(f"Image {i * 100}:")
    print(f"Predicted Class: {predicted_class} ({cifar100_labels[predicted_class]})")
    print(f"True Class: {true_class} ({cifar100_labels[true_class]})")
    print(f"Confidence: {confidence:.4f}")
    print("Prediction is 1!" if predicted_class == true_class else "Prediction is 0.")
    print()

# Create and display DataFrame
results_df = pd.DataFrame(results)
print("\nPrediction Results Table:")
print(results_df)

# Optionally save to CSV
results_df.to_csv('/content/prediction_results.csv', index=False)  # Save to Google Drive

import pandas as pd

# Read the CSV file (replace 'data.csv' with your file name)
df = pd.read_csv('prediction_results.csv')

# Assume the boolean column is named 'value' (replace with your column name)
column_name = 'Correct'

# Calculate the percentage of True values
true_count = df[column_name].sum()  # True is treated as 1, False as 0
total_count = len(df)
true_percentage = (true_count / total_count) * 100

# Print the result
print(f"Percentage of True values: {true_percentage:.2f}%")

import matplotlib.pyplot as plt
import numpy as np

def plot_train_val_loss(train_loss, val_loss, epochs=None):
    if epochs is None:
        epochs = range(1, len(train_loss) + 1)
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, train_loss, label='Training Loss', marker='o')
    plt.plot(epochs, val_loss, label='Validation Loss', marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training vs Validation Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_train_val_loss(history.history['loss'], history.history['val_loss'])

import matplotlib.pyplot as plt
import numpy as np

def plot_train_val_loss(train_loss, val_loss, epochs=None):
    if epochs is None:
        epochs = range(1, len(train_loss) + 1)
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, train_loss, label='Training accuracy', marker='o')
    plt.plot(epochs, val_loss, label='Validation accuracy', marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training vs Validation accuracy')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_train_val_loss(history.history['accuracy'], history.history['val_accuracy'])

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

# Initialize lists to store results
results = []

# Load the best model (assuming it's saved with all layers, including custom objects if any)
#model = tf.keras.models.load_model('/content/googlenet.h5')


for i in range(100):
    # Get the test image and its true label
    image = x_test[i * 100]  # Shape: (224, 224, 3) after resizing
    true_class_one_hot = y_test[i * 100]  # Shape: (100,) - one-hot encoded vector
    true_class = np.argmax(true_class_one_hot)  # Get the index of the true class

    # Add batch dimension to the image
    image = np.expand_dims(image, axis=0)  # Shape: (1, 224, 224, 3)

    # Make prediction (model returns three outputs)
    predictions = model_with_custom_pool.predict(image)  # Returns a list: [main_output, aux_output_1, aux_output_2]
    predicted_class = np.argmax(predictions, axis=1)[0]  # Use main_output for final prediction
    confidence = np.max(predictions)  # Confidence from main_output

    # Store results in dictionary
    results.append({
        'Image_Index': i * 100,
        'Predicted_Class': cifar100_labels[predicted_class],
        'True_Class': cifar100_labels[true_class],
        'Confidence': f"{confidence:.4f}",
        'Correct': predicted_class == true_class
    })

    # Visualize the image
    plt.imshow(x_test[i * 100])  # Show original 32x32 image for visualization
    plt.title(f"Predicted: {cifar100_labels[predicted_class]}, True: {cifar100_labels[true_class]}")
    plt.show()

    # Print results
    print(f"Image {i * 100}:")
    print(f"Predicted Class: {predicted_class} ({cifar100_labels[predicted_class]})")
    print(f"True Class: {true_class} ({cifar100_labels[true_class]})")
    print(f"Confidence: {confidence:.4f}")
    print("Prediction is 1!" if predicted_class == true_class else "Prediction is 0.")
    print()

# Create and display DataFrame
results_df = pd.DataFrame(results)
print("\nPrediction Results Table:")
print(results_df)

# Optionally save to CSV
results_df.to_csv('/content/prediction_results.csv', index=False)  # Save to Google Drive

import pandas as pd

# Read the CSV file (replace 'data.csv' with your file name)
df = pd.read_csv('prediction_results.csv')

# Assume the boolean column is named 'value' (replace with your column name)
column_name = 'Correct'

# Calculate the percentage of True values
true_count = df[column_name].sum()  # True is treated as 1, False as 0
total_count = len(df)
true_percentage = (true_count / total_count) * 100

# Print the result
print(f"Percentage of True values: {true_percentage:.2f}%")